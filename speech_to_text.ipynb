{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech to Text\n",
    "\n",
    "Converting speech to text is a powerful tool in higher education, making spoken content accessible and analyzable. This technology enables automatic transcription of lecture recordings, creates real-time captions for classroom discussions, and helps international students follow along with spoken English. It also supports research by transcribing interviews, focus groups, and oral histories. By converting speech to text, institutions can improve accessibility, support language learners, and create searchable archives of educational content.\n",
    "\n",
    "Using models from [HuggingFace](https://huggingface.co/docs), we compare two speech recognition approaches:\n",
    "- [Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2)\n",
    "- [Whisper](https://huggingface.co/openai/whisper-tiny.en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "from transformers import (\n",
    "    Wav2Vec2ForCTC, \n",
    "    Wav2Vec2Processor,\n",
    "    WhisperProcessor, \n",
    "    WhisperForConditionalGeneration\n",
    ")\n",
    "\n",
    "def validate_speech_samples_path(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Validate that the speech samples directory exists and contains .wav files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    path : str\n",
    "        Path to the speech samples directory\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Validated path to speech samples directory\n",
    "        \n",
    "    Raises:\n",
    "    -------\n",
    "    RuntimeError\n",
    "        If the directory doesn't exist or contains no .wav files\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise RuntimeError(\n",
    "            f\"Speech samples directory not found at: {path}\\n\"\n",
    "            \"Please make sure the 'speech_samples' directory exists in the same \"\n",
    "            \"directory as this notebook.\"\n",
    "        )\n",
    "    \n",
    "    wav_files = glob.glob(os.path.join(path, \"*.wav\"))\n",
    "    if not wav_files:\n",
    "        raise RuntimeError(\n",
    "            f\"No .wav files found in: {path}\\n\"\n",
    "            \"Please make sure the directory contains .wav files.\"\n",
    "        )\n",
    "    \n",
    "    return path\n",
    "\n",
    "# Define and validate path to speech samples relative to notebook location\n",
    "SCRIPT_DIR = os.path.dirname(os.path.abspath('__file__'))\n",
    "SPEECH_SAMPLES_PATH: str = validate_speech_samples_path(\n",
    "    os.path.join(SCRIPT_DIR, 'speech_samples')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Wav2Vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_file(file_path: str, sample_rate: int = 16000) -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"\n",
    "    Load and preprocess an audio file for speech-to-text conversion.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the audio file to be loaded\n",
    "    sample_rate : int, optional\n",
    "        Target sampling rate for the audio, defaults to 16000 Hz\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple[np.ndarray, int]\n",
    "        A tuple containing:\n",
    "        - The loaded audio data as a numpy array\n",
    "        - The sampling rate used\n",
    "    \"\"\"\n",
    "    try:\n",
    "        audio, rate = librosa.load(file_path, sr=sample_rate)\n",
    "        return audio, rate\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading audio file {file_path}: {str(e)}\")\n",
    "\n",
    "def get_audio_files(directory: str, extension: str = \"*.wav\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Get a sorted list of audio files from a directory.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    directory : str\n",
    "        Path to the directory containing audio files\n",
    "    extension : str, optional\n",
    "        File extension pattern to match, defaults to \"*.wav\"\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[str]\n",
    "        Sorted list of file paths matching the extension pattern\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return sorted(glob.glob(os.path.join(directory, extension)))\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error accessing directory {directory}: {str(e)}\")\n",
    "\n",
    "def create_results_dataframe(wav_files: List[str], transcriptions: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a DataFrame to store speech-to-text results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    wav_files : List[str]\n",
    "        List of audio file paths\n",
    "    transcriptions : List[str]\n",
    "        List of transcribed text corresponding to the audio files\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing wav_input and txt_output columns\n",
    "    \"\"\"\n",
    "    return pd.DataFrame({\n",
    "        'wav_input': wav_files,\n",
    "        'txt_output': transcriptions\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def transcribe_with_wav2vec2(audio_files: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transcribe audio files using the Wav2Vec2 model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    audio_files : List[str]\n",
    "        List of paths to audio files to transcribe\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing original audio files and their transcriptions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize Wav2Vec2 model and processor\n",
    "        processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        \n",
    "        wav_files = []\n",
    "        transcriptions = []\n",
    "        \n",
    "        for filename in audio_files:\n",
    "            try:\n",
    "                # Load and process audio\n",
    "                audio, rate = load_audio_file(filename)\n",
    "                input_values = processor(\n",
    "                    audio, \n",
    "                    return_tensors=\"pt\", \n",
    "                    sampling_rate=rate\n",
    "                ).input_values\n",
    "                \n",
    "                # Generate predictions\n",
    "                with torch.no_grad():\n",
    "                    logits = model(input_values).logits\n",
    "                prediction = torch.argmax(logits, dim=-1)\n",
    "                \n",
    "                # Decode prediction to text\n",
    "                transcription = processor.batch_decode(prediction)[0]\n",
    "                \n",
    "                # Store results\n",
    "                wav_files.append(filename)\n",
    "                transcriptions.append(transcription)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        return create_results_dataframe(wav_files, transcriptions)\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error initializing Wav2Vec2 model: {str(e)}\")\n",
    "\n",
    "# Process audio files with Wav2Vec2\n",
    "audio_files = get_audio_files(SPEECH_SAMPLES_PATH)\n",
    "df_wav2vec2 = transcribe_with_wav2vec2(audio_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_with_whisper(audio_files: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transcribe audio files using the OpenAI Whisper model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    audio_files : List[str]\n",
    "        List of paths to audio files to transcribe\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing original audio files and their transcriptions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize Whisper model and processor\n",
    "        processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "        model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "        \n",
    "        wav_files = []\n",
    "        transcriptions = []\n",
    "        \n",
    "        for filename in audio_files:\n",
    "            try:\n",
    "                # Load and process audio\n",
    "                audio, rate = load_audio_file(filename)\n",
    "                \n",
    "                # Process audio with attention mask\n",
    "                inputs = processor(\n",
    "                    audio, \n",
    "                    return_tensors=\"pt\", \n",
    "                    sampling_rate=rate,\n",
    "                    return_attention_mask=True  # Explicitly request attention mask\n",
    "                )\n",
    "                \n",
    "                # Generate predictions with attention mask\n",
    "                with torch.no_grad():\n",
    "                    predicted_ids = model.generate(\n",
    "                        inputs.input_features,\n",
    "                        attention_mask=inputs.attention_mask\n",
    "                    )\n",
    "                \n",
    "                # Decode prediction to text\n",
    "                transcription = processor.batch_decode(\n",
    "                    predicted_ids, \n",
    "                    skip_special_tokens=True\n",
    "                )[0]\n",
    "                \n",
    "                # Store results\n",
    "                wav_files.append(filename)\n",
    "                transcriptions.append(transcription)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        return create_results_dataframe(wav_files, transcriptions)\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error initializing Whisper model: {str(e)}\")\n",
    "\n",
    "# Process audio files with Whisper\n",
    "audio_files = get_audio_files(SPEECH_SAMPLES_PATH)\n",
    "df_whisper = transcribe_with_whisper(audio_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(df: pd.DataFrame, model_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Display transcription results in a formatted way.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing transcription results\n",
    "    model_name : str\n",
    "        Name of the model used for transcription\n",
    "    \"\"\"\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wav2Vec2 Results:\n",
      "--------------------------------------------------------------------------------\n",
      "                                                                       wav_input                    txt_output\n",
      "  /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.apple.wav   LOOK AT THE WONDERFUL APPLE\n",
      " /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.apples.wav   LOOK AT THE WONDERFUL APPLE\n",
      "   /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.baby.wav    LOOK AT THE BEAUTIFUL BABY\n",
      "  /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.babys.wav  LOOK AT THE BEAUTIFUL BABIES\n",
      "   /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.bike.wav    LOOK AT THE WONDERFUL BIKE\n",
      "  /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.bikes.wav   LOOK AT THE WONDERFUL BIKES\n",
      " /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.cookie.wav  LOOK AT THE WONDERFUL COOKIE\n",
      "/Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.cookies.wav LOOK AT THE WONDERFUL COOKIES\n",
      "  /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.doggy.wav   LOOK AT THE BEAUTIFUL DOGGY\n",
      " /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.doggys.wav LOOK AT THE BEAUTIFUL DOGGIYS\n",
      "  /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.kitty.wav   LOOK AT THE BEAUTIFUL KITTY\n",
      " /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.kittys.wav LOOK AT THE BEAUTIFUL KITTIES\n",
      "  /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.truck.wav   LOOK AT THE WONDERFUL TRUCK\n",
      " /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.trucks.wav  LOOK AT THE WONDERFUL TRUCKS\n",
      " /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.turtle.wav  LOOK AT THE BEAUTIFUL TURTLE\n",
      "/Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.turtles.wav LOOK AT THE BEAUTIFUL TURTLES\n"
     ]
    }
   ],
   "source": [
    "# Display results for Wav2Vec2\n",
    "display_results(df_wav2vec2, \"Wav2Vec2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Whisper Results:\n",
      "--------------------------------------------------------------------------------\n",
      "                                                                       wav_input                      txt_output\n",
      "  /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.apple.wav    Look at the wonderful apple.\n",
      " /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.apples.wav   Look at the wonderful apples.\n",
      "   /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.baby.wav     Look at the beautiful baby.\n",
      "  /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.babys.wav   Look at the beautiful babies.\n",
      "   /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.bike.wav     Look at the wonderful bike.\n",
      "  /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.bikes.wav    Look at the wonderful bikes.\n",
      " /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.cookie.wav   Look at the wonderful cookie.\n",
      "/Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.cookies.wav  Look at the wonderful cookies.\n",
      "  /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.doggy.wav    Look at the beautiful doggy.\n",
      " /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.doggys.wav  Look at the beautiful doggies!\n",
      "  /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.kitty.wav    Look at the beautiful kitty!\n",
      " /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.kittys.wav  Look at the beautiful kitties!\n",
      "  /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.truck.wav    Look at the wonderful truck.\n",
      " /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.trucks.wav   Look at the wonderful trucks.\n",
      " /Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.turtle.wav   Look at the beautiful turtle.\n",
      "/Users/tereuter/Desktop/github/NLP-speech-to-text/speech_samples/the.turtles.wav  Look at the beautiful turtles.\n"
     ]
    }
   ],
   "source": [
    "# Display results for Whisper\n",
    "display_results(df_whisper, \"Whisper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The comparison between Whisper and Wav2Vec2 reveals several key advantages of the Whisper model:\n",
    "\n",
    "1. Performance\n",
    "   - Approximately 20% faster transcription speed\n",
    "   - Potential for further performance optimization\n",
    "\n",
    "2. Accuracy\n",
    "   - Better handling of singular/plural forms (e.g., \"apple\" vs. \"apples\")\n",
    "   - More accurate spelling (e.g., \"doggies\" vs. \"DOGGIYS\")\n",
    "\n",
    "3. Nuanced Output\n",
    "   - Enhanced punctuation handling including emphatic marks\n",
    "   - Better preservation of emotional context through punctuation\n",
    "   - Improved potential for downstream tasks like sentiment analysis\n",
    "\n",
    "These differences make Whisper particularly suitable for applications requiring:\n",
    "- High transcription accuracy\n",
    "- Preservation of emotional context\n",
    "- Integration with sentiment analysis pipelines\n",
    "- Real-time or near-real-time processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
